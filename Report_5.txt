Training has been successfully demonstrated using my framework (files DBN_writeparams.py and DBN_example). For this initial test, the network used two layers of width 30, a batch size of 10, an output of 10 classes, 1000 pretraining epochs, 3000 finetuning epochs, one step of Gibbs sampling to compute each level of representation, and only the first 71500 pixels of the Pavia Centre image (out of 1.2 million). The framework so far consists of the following methods:

DBN_writeparams:
A modification of the Theano module, DBN, this program integrates a function into the DBN class that returns the matrices of trained weights and biases for each layer (function "return_params"). These parameters are then printed in the final stage of execution after training is complete and are then written to a plain text file (.txt) using the Numpy function "savetxt", which conveniently preserves the structure of the matrices by delimiting the rows with linefeeds rather than single spaces. The advantage of this preservation is that it allows the testing module (DBN_example) to read the trained parameters with a single function call (Numpy's "loadtxt") without having to rebuild the matrices.

DBN_example:
DBN_example has successfully demonstrated the loading of (empty) parameters produced by DBN_writeparams, as well as the feed-forward propagation of a small portion of the Pavia Centre testing image, after which the high-level features and class vectors were successfully extracted (they were all zero for this initial demonstration). The next step in testing this module is to load the fully trained parameters and to propagate a larger portion of the testing image for classification.
I developed this module to be independent of the primary Theano DBN class, and it thus imports the class defined in DBN_writeparams, which contains the functions needed for loading and printing trained parameters, as well as images and their ground truths. Currently, those functions are in the process of being further modularized, with the intention of maintaining a cleanly framework. (So far, this cleanliness has been less of a priority than the debugging of the framework's functionality. As a consequence, the function that loads images for training/testing are extraneously repeated between the two modules for now.) 

The two modules have been extensively annotated and provide many printed outputs to ensure that the methods succeed at every stage. These include the dimensions of parameters and of the training, testing, and validation sets, as well as the contents of these loaded matrices.

Currently, one obstacle is a limitation in memory, which prevents the full-scale training or classification of the Pavia Centre image. However, I expect that both the training and the testing can be performed piecewise due to the ability of Python to save arrays in plain text files while retaining their structure, as described above. Further experimentation and debugging will be necessary to ensure the feasibility of this piecewise approach.


Another notable concern is the explosion of the reconstruction error in pretraining the first layer of the network. I have attempted to change every detail of the DBN object to prevent this (the widths, depth, Gibbs steps, batch size, test/valid/train ratio, epochs, learning rates, etc.), but the first layer continues to form poor reconstructions, at least when using cross entropy as a metric, which essentially measures the disparity between the visible data and the reconstruction of the data generated by a single Gibbs step (see Hinton's "Practical Guide to Training RBMs" for a formal definition). Upon debugging the calculation of this cross entropy, I found that the method works as it should. Further investigation may be needed, but for the time being, I have taken Hinton's advice to ignore the cross entropy until I know for certain (after testing) that the trained parameters of the first layer are inappropriately large. In contrast, the second layer shows desirable pretraining behavior, with the reconstruction error approaching zero very closely after 1000 epochs.
